\chapter{Evaluation}
\label{c:discussion}
The research question that is tackled in this thesis has two primary objectives.
The first goal is to provide a visualization of program behavior at the abstraction level of objects that preservers the immediate character and low memory footprint of the underlying tracing framework.
The second goal is to provide a visualization that imparts the underlying information in a perceivable manner and supports the developer in the understanding of the software system at hand.

Section \ref{s:DiscussionPerformance} gives an insight to which extent the first goal could be reached with the prototypic implementation of our approach.
To validate that the extensions that had to be made to the tracing framework and the computational complexity introduced through the visualization do not jeopardize immediacy, an evaluation of the runtime performance of our approach is given in Section \ref{ss:DiscussionPerformance}.
An evaluation of the additional memory consumption that our specialized variant of  step-wise run-time analysis brings along is presented in Section \ref{ss:DiscussionSpace}. 
The achievement of the second goal is examined with the help of a user evaluation which is presented in Section \ref{s:DiscussionEvaluation}.

In addition, the limitations of our approach are explained in Section \ref{s:DiscussionLimitations}, and finally the applicability of our approach to other programming languages and environments is discussed in Section \ref{s:DiscussionApplicability}.

\section{Performance Evaluation}
\label{s:DiscussionPerformance}
Perscheid et al. already have shown that the step-wise run-time analysis approach that serves as a basis of \textsc{PathObjects} enables the immediate exploration of a programs run-time while maintaining a low memory footprint  \cite{perscheid_immediacy_2010}.
To ensure that the computational extra work required for visualizing recorded object interactions preserves the immediate character of this approach, an evaluation of the runtime performance of our implementation is given in Section \ref{ss:DiscussionPerformance}.
Since \textsc{PathObjects} also introduces additional tracing effort, an analysis of its memory requirements is presented in Section \ref{ss:DiscussionSpace}.
The projects that have been used to conduct those evaluations are presented in Section \ref{ss:DiscussionProjects}.

\subsection{Projects}
\label{ss:DiscussionProjects}
In order to get a meaningful performance estimation, four real-world software projects with different backgrounds were chosen as a basis for the measurements.
The extent of those projects (cf. Table \ref{t:EvaluationProjects}) ranges from roughly 210 methods in 16 classes to almost 1900 methods in over 210 classes.
\textsc{Seaside}\footnote{http://seaside.st, last checked \today} is a full-fledged, industry grade web application framework.
\textsc{DicThesauruasRex}\footnote{https://www.hpi.uni-potsdam.de/hirschfeld/trac/SqueakCommunityProjects/wiki/dicThesaurusRex. last checked \today} is the result of an undergraduate student project which adds spelling correction and synonym search facilities to the Squeak development environment.
\textsc{SUnit}\footnote{http://sunit.sourceforge.net/, last checked \today} is the de-facto standard for unit testing frameworks in Smalltalk environments and constitutes a landmark of test-driven development.
The \textsc{System Browser}\footnote{http://wiki.squeak.org/squeak/673, last checked \today} is the fundamental development tool in Squeak images that allows to browse and edit the source codes of the image.

\begin{table}
\centering
\begin{tabular}{lcccccc}
\toprule[1.5pt]
\phantom{abc} & \phantom{abc} & \multicolumn{2}{c}{Classes} & \phantom{abc} & \multicolumn{2}{c}{Methods}  \\
\cmidrule{3-4} \cmidrule{6-7}
Project    && System & Test && System & Test \\
\midrule
\textsc{Seaside-Core}		&&	163	&	49	&&	1409	&	458	\\
\textsc{DicThesaurusRex}	&&	23	&	14	&&	204		&	69	\\
\textsc{SUnit}				&&	8	&	8	&&	160		&	49	\\
\textsc{System Browser}		&&	54	&	13	&&	1204	&	162	\\
\bottomrule[1.5pt]
\end{tabular}
\caption[Test Subjects]{Scale of the software systems that served as a basis for the performance evaluation.}
\label{t:EvaluationProjects}
\end{table}

\subsection{Runtime Performance}
\label{ss:DiscussionPerformance}
The major bottleneck in our approach of the rendering of object interactions can be accounted for by the hierarchical graph drawing that is performed during the visualization phase.
\todo{point out seq triv.}
Although the applied algorithm has been designed with its interactive application in mind and generally performs fast enough therefor \cite{gansner_technique_1993}, its runtime is heavily dependent on the specific structure of the graph at hand.
Precisely, the edge crossing minimization problem is NP-hard and the applied heuristic requires quadratic time in the number of nodes in the worst case \cite{tamassia_handbook_2013}, which eventuates when edges span the maximum possible number of ranks.
An example graph with $n$ ranks that shows this characteristic is depicted in Figure \ref{fig:graph-worst-case}.
Since it is practically impossible to predict to which degree call trees of test cases conform to this structure, an empirical study has been carried out with real-world software systems to examine the runtime behavior of our approach.

\begin{figure}[b]
	\centering	
	\digraph
	[scale=0.8]{worstCaseRuntimeComplexity}
	{
		margin=0
		nodesep=0.4
		node [style=rounded, shape=box, fontsize=11, height=0.4]
		n1 [label=<N<SUB>1</SUB>>]
		n2 [label=<N<SUB>2</SUB>>]
		nx [label="...", style="rounded,dotted"]
		nn1 [label=<N<SUB>n-1</SUB>>]
		nn [label=<N<SUB>n</SUB>>]
		n1->n2
		n2 -> nx [style=dotted,arrowhead=onormal]
		nx->nn1 [style=dotted,arrowhead=onormal]
		nx -> nn [style=dotted, arrowhead=onormal]
		nn1-> nn
		n1 -> nn
		n2 -> nn
		{rank=same n1 n2 nx nn1 nn}
	}
	\caption[Example Graph that Entails Worst Case Runtime Complexity]{Example graph that entails the worst case runtime complexity of $\mathcal O(n^2)$.}
	\label{fig:graph-worst-case}
\end{figure}

\subsubsection{Test Arrangement}
\todo[inline]{Missing}

\subsubsection{Test Environment}
All measurements have been performed on an Intel\copyright{} Core\texttrademark{}  i7-2620M CPU @ 2.70GHz.
The Squeak image version 4.4-12327 has been executed with the Croquet Closure Stack VM, version StackInterpreter VMMaker-oscog-EstebanLorenzano.237\footnote{http://source.squeak.org/VMMaker/VMMaker-oscog-EstebanLorenzano.237.mcz, last checked \today}. 
This combination of CPU and VM performs at 528.1 million bytecodes/sec and 27.3 million sends/sec in the Squeak Tiny Benchmarks. Graphviz has been available in version 2.34.

\subsubsection{Results}
The results of the measurement are depicted in Figure \ref{f:DiscussionRuntime}.
The start-up time is faster than one second in more than 50\% of all measured test cases.
More than 75\% of all measurements are below two seconds.
In the case of \textsc{DicThesaurusRex} and \textsc{SUnit}, even the third quartiles are below one second.
The outliers reach up to 34 seconds with the worst-case occurring in the \textsc{Seaside-Core} project.

In comparison to the majority of test cases, the start-up times of the outliers are significantly slower.
However, when compared to traditional dynamic analysis tools (cf. Section \ref{ss:BackgroundAnalysisProblems}), the results nevertheless are available considerably faster.
Furthermore, those test case are rather comprehensive in terms of the number of objects and messages, and it is doubtful that developers want to visualize these amounts of data in their entirety.
Consequently, a possible solution to speed up the visualization of these test cases would be to provide an alternative entry point that allows developers to specify entities of interest through the \textsc{PathView} filter.
Thus, the graph that has to be processed during the graph drawing phase could be narrowed down before the first layout computation.

In summary, it can be said that the total time required for tracing and processing is entirely satisfactory in case of the four investigated projects.
Since these projects are real-world software systems rather than synthetic benchmarks, it can be concluded that the approach is suitable to achieve considerably better performance compared to traditional dynamic analysis tools.

\begin{figure}[b!]
	\centering
	\includegraphics[width=0.9\textwidth]{../plots/05-Runtimes}
	\caption[Runtime Performance of \textsc{PathObjects}]{Runtime performance of \textsc{PathObjects} with various projects (whiskers indicate 2.5th and 97.5th percentiles).}
	\label{f:DiscussionRuntime}
\end{figure}

\subsubsection{Threats to Validity}
The runtime measurements have been performed with the help of the \inlinecode{BlockClosure>>timeToRunWithoutGC} method, which has two implications.
First, the precision of the measurement is limited by the precision of the millisecond clock of the Squeak image.
Another side effect of measuring clock time instead of CPU time is that other running operating system processes can have an effect on the results.
But as millisecond precision is not important for our evaluation, both limitations are acceptable.

The second implication is that in practice, the start-up time of \textsc{PathObjects} will be slightly worse than depicted by the results of the runtime evaluation.
However, garbage collection should not be of major importance during the start-up phase.
Objects are not created and destroyed excessively through the tracing framework, so the main parts that are qualified for garbage collection are the ones of the system under investigation.
Furthermore, the start-ups are short running tasks.
Even if garbage collection would account for a significant share of the runtime, the overall start-up time would still be feast enough in most cases.

\subsection{Space Consumption}
\label{ss:DiscussionSpace}
As pointed out in Section \ref{s:BackgroundAnalysis}, the major problem of dynamic analysis techniques is the huge amount of data arising during tracing runs.
Perscheid et al. have proposed the \emph{Step-wise Run-time Analysis} approach \cite{perscheid_immediacy_2010} as tracing technique with considerably lowered space requirements. 
This approach also serves as a basis of \textsc{PathObjects}, but additional tracing effort had to be introduced to allow for the reconstruction of object interaction diagrams from those traces.
For that reason, an estimation of the additional space consumption \textsc{PathObjects} adds on top of the underlying tracing framework is presented in this section.

\subsubsection{Preliminary Considerations}
The main memory consumption of a specific part of the system is hard to measure exactly in a garbage collected environment like the Squeak virtual machine.
For that reason, the space requirements are estimated by means of theoretical deduction.
Figure \ref{fig:DiscussionStructure} depicts the basic components \textsc{PathObjects} traces consists of in the form of a simplified class diagram.

Each occurring object is represented by a proxy object that basically consists of an integer identifier and a pointer to its type.
Sent messages are represented by instances of the \inlinecode{Message} class.
They consist of pointers to the preceding and succeeding messages as well as a collection of integer object identifiers.
This collection includes identifiers for the sender and receiver of a message, its arguments, and for its return value.
In case of the \inlinecode{ObjectProxy} and \inlinecode{Message} classes, there is an overhead of two words for each object instance consumed by the object header.
Since all attributes are either integer values or pointers to other objects, the size of each attribute can be assumed to be one word.
The size of the \inlinecode{involvedObjects} collection has a constant part of four words for object headers and attributes and three words for the identifiers for sender, receiver, and return value, but ultimately depends on the number of arguments of the according message.
In other words, the space consumption of our traces is dependent on the number of objects, the number of messages, and the number of arguments of each message.

Taken as a whole, the space consumption (in words) can be expressed as a function $s(o,m)$ of the number of objects $o$ and the number of messages $m$:
\begin{equation}
s(o,m)=o*4 + m*5 + \sum_{n=1}^{m} (7 + a(M_n))\label{eq:SpaceConsumptionWords}
\end{equation}
Thereby, the number of arguments of a message $M_n$ are represented by the function $a$.
Since a word consumes four bytes in the virtual machine we used, the total space consumption in kilobytes $s_{kB}$ can be calculated with the help of formula \ref{eq:SpaceConsumptionBytes}. 
\begin{equation}
s_{kB}(o,m) = \frac{4 * s(o,m)}{1024}\label{eq:SpaceConsumptionBytes}
\end{equation}
To get a reasonable estimation of the three variable parts $o$, $m$, and $a(M_n)$, their actual numbers in the case of the four evaluation projects have been measured.

\usetikzlibrary{positioning,shapes,shadows,arrows}
\tikzstyle{class}=[
	rectangle,
	draw=black,
	text centered,
	anchor=north,
	text=black,
	text width=3cm,
	minimum height=0.6cm,
	shading=axis,bottom color=black!20,top color=white,shading angle=45
	]
	
\tikzset{every node/.style={draw}}

\begin{figure}[b!]
	\centering
	\begin{tikzpicture}[node distance=2cm, font=\sffamily]

		\node (ObjectProxy) [class, rectangle split, rectangle split parts=2] {
				\textbf{ObjectProxy}
				\nodepart{second} objectId \\ type
		};

		\node (Message) [class, rectangle split, rectangle split parts=2, below right=0cm and 1.5cm of ObjectProxy.north east] {
			\textbf{Message}
			\nodepart{second} involvedObjects \\ previous \\ next
		};
		
		\node (Collection) [class, rectangle split, rectangle split parts=2, below right=0.6cm and 2cm of Message.north east] {
			\textbf{Collection\vphantom{g}}
			\nodepart{second} senderId \\ receiverId \\ argumentId 1 \\ ... \\
			 argumentId n \\ resultId
		};
		
		\draw[>-, >=diamond, shorten >=1pt, thick] 
    		(Message.east) ++(0,0.2) -| ([yshift=1.5cm]Collection.west); 
			
	\end{tikzpicture}
	\caption[Memory Consuming Components of \textsc{PathObjects} Traces]{Memory consuming components of \textsc{PathObjects} traces.}
	\label{fig:DiscussionStructure}
\end{figure}

\subsubsection{Results}
\todo[inline]{Missing}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.9\textwidth]{../plots/05-SpaceConsumption}
	\caption[Space Requirements Introduced Through \textsc{PathObjects}]{Additional space consumption introduced through the tracing requirements of \textsc{PathObjects} (whiskers indicate 2.5th and 97.5th percentiles).}
	\label{f:DiscussionSpace}
\end{figure}

\subsubsection{Threats to Validity}
The calculated memory consumption should be interpreted as lower limit of the values that are reached in practice.
Although the overhead implicated by object instances themself in the Squeak virtual machine is taken into consideration, the memory consumption will effectively be higher than determined by our calculations.
First, the memory overhead the virtual machine itself introduces to manage objects can not be considered with the chosen approach.
Second, memory allocation is rather performed in blocks than with byte precision.
Consequently, the actual memory consumption depends on the number of blocks that have to be allocated to satisfy the requirements.
Third, a similar problem is memory alignment or rather data misalignment, which can cause objects to consume more memory than necessary.
These pitfalls of memory management can not be taken into consideration reasonably with our theoretical deduction.

Nonetheless, all those factors will not add up to invalidate the basic results of the evaluation.
Even when assuming huge deviations that lead to a tenfold memory consumption compared to the result of formula \ref{eq:SpaceConsumptionWords}, the consumption still would lie within a range of a few megabytes.
Nowadays, this is a scale that can be demanded from hardware with clear conscience.

\clearpage
\section{User Evaluation}
\label{s:DiscussionEvaluation}
In order to verify that our proposed solution can help developers to comprehend software systems and their behavior faster and more comprehensive than with the help of traditional tools alone, a user evaluation has been performed.
Study participants were challenged to answer questions which reportedly arise often during software development about a system previously unknown to them.
The study compares how fast and how correct the participants could answer the questions  when using either the tools provided by the standard Squeak environment alone, or the other tools of the \textsc{PathTools} suite in addition, or when using \textsc{PathObjects} in addition.

The chapter is structured as follows:
Section \ref{ss:DiscussionEvaluationParticipants} outlines the selection of the study participants and their professional background.
Section \ref{ss:DiscussionEvaluationQuestions} gives an overview over the questions that were asked and the reasons for their selection.
Section \ref{ss:DiscussionEvaluationExecution} describes under which circumstances the study was conducted.
Section \ref{ss:DiscussionEvaluationResults} depicts the results of the evaluation and finally Section \ref{ss:DiscussionEvaluationThreats} covers potential threats to the validity of our results.

\subsection{Probands}
\label{ss:DiscussionEvaluationParticipants}

\subsection{Questions}
\label{ss:DiscussionEvaluationQuestions}

\subsection{Execution}
\label{ss:DiscussionEvaluationExecution}

\subsection{Results}
\label{ss:DiscussionEvaluationResults}

\subsection{Threats to Validity}
\label{ss:DiscussionEvaluationThreats}

\clearpage
\section{Discussion}
\label{s:DiscussionLimitations}
The operation of our prototype and the underlying tracing mechanism take some preconditions for granted whose non-compliance could potentially render them non-functional.
Those requirements are depicted in the following sections.
The restrictions that apply when dealing with systems that use the meta-programming and reflection capabilities of the Squeak environment are described in Section \ref{ss:DiscussionLimitationsMeta}.
The importance of test coverage is presented in Section \ref{ss:DiscussionLimitationsCoverage}.
Finally, the reliance on test quality is depicted in Section \ref{ss:DiscussionLimitationsTestQuality}.
\todo{Rephrase}

\subsection{Metaprogramming and Reflection}
\label{ss:DiscussionLimitationsMeta}
The utilization of metaprogramming and reflection capabilities within the system under observation does not necessarily pose a problem, neither on the conceptual nor on the technical level.
However, the underlying tracing approach based on method wrappers is not very resistant to structural changes that can be caused by the usage of those capabilities.

For instance, our prototypical implementation collects values of the program counter during tracing in order to permit the highlighting of the current message send within the corresponding method.
For that purpose, it browses the call stack, and expects a specific structure while doing so.
All method calls that originate from the tracing framework have to be skipped in order to find the actual call context and thus the appropriate program counter value.
If a program under observation manipulates the call stack, for instance by using method wrappers itself, the assumptions made about the sequence of tracing framework calls no longer hold true.
Consequently, information that is collected during such tracing runs would be incorrect.

Other tracing approaches might be less susceptible to such interferences, especially if traces are built via external observation of the target system, as is the case with the utilization of debug interfaces or with tracing on the virtual machine level.
However, the application of that sort of reflective capabilities is rarely found in practice.
Therefore, the limitations that are introduced in conjunction with tracing on the basis of method wrappers are acceptable.
Furthermore, these limitations do not invalidate the concept of \textsc{PathObjects} itself, since it might just as well be implemented with other, more resistant tracing approaches.

\subsection{Reliance on Test Coverage}
\label{ss:DiscussionLimitationsCoverage}
Since only test cases can serve as reproducible entry points in our current prototypic implementation, only those parts of a system can be visualized that are covered by at least one test case.
In practice, this means that not all entities and execution branches that are actually in use during productive executions of the system can necessarily be visualized with existing tests.
Nonetheless, it is also feasible to write specific unit tests for the soul purpose of visualization.
Such a test then can be seen as the description of a scenario that the user wishes to examine through our tool.
Admittedly, this strategy requires the user to already have a certain degree of knowledge, which might not apply when dealing with previously unknown systems.
In such cases, one might not be able to construct a working scenario.

However, the concept of \textsc{PathObjects} is not limited to test cases.
The underlying tracing approach works with any reproducible entry point into program behavior.
Consequently, the restriction to test cases is a limitation of the prototypical implementation rather than of the concept itself.

\subsection{Reliance on Test Quality} \todo{missing refs}
\label{ss:DiscussionLimitationsTestQuality}
The best practices for unit testing and the characteristics unit tests should display enjoy broad consent throughout the agile software development community.
To name a few, they are supposed to be isolated, to be free of side-effects, to run fast, and to be reproducible, automated and unique \cite{meszaros_xunit_2006, beck_test_2002}.
However, when tests are being used as entry points for \textsc{PathObjects}, two of those recommendations become requirements.

Fast execution times are a key requirement for the applicability of our approach.
If, for instance, a test would take minutes to execute, the user would have to wait that long every time a refinement run has to be performed to collect previously unknown information.
This would possibly be still acceptable if such runs were executed only occasionally, but \textsc{PathObjects} encourages the continuous use of those features.
For example, as depicted in Section \ref{missing}, refinement runs are performed automatically as long as an object state inspector is expanded.
That means that with every step to a previously unvisited point of the execution trace, the underlying test case gets executed automatically, which in turn induces waiting time for the user in the case of long running tests. The good news is that in practice, tests usually run fast enough to make immediate feedback possible \cite{perscheid_immediacy_2010}.

The second attribute our approach presupposes is strict determinism.
Again, the main cause of concern are refinement runs.
If a test case follows divergent branches or produces objects with varying states in repeated executions, the results that are returned from such runs may be incorrect or misleading.
Admittedly, there are legitimate cases where tests are not completely deterministic, for instance when testing multithreaded applications or when working with current dates and times.
However, the \emph{record and replay} technique has been proposed to tackle this problem \cite{choi_deterministic_1998} and has successfully been adapted to the step-wise runtime analysis approach \cite{felgentreff_comparison_2012}.

\subsection{Applicability to other Programming Languages}
\label{s:DiscussionApplicability}
The fact that the prototypical implementation of our approach targets an environment that leads a niche existence raises the question to which extent the approach is transferable to other programming languages and environments.
To answer this question, one can consider the requirements \textsc{PathObjects} is based on.

First and foremost, the approach is specifically tailored to object-oriented systems.
However, a programming language does not necessarily have to feature object-oriented concepts to the same comprehensive extent that Smalltalk exhibits in order to qualify for the application of our approach.
For instance, most popular object-oriented programming languages do not expose classes as objects conceptually. 
Nevertheless, classes can readily be depicted as objects (or generally speaking: senders and receivers of messages) without impairing the expressiveness and comprehensibility of our proposed visualization.
The second requirement is the existence of reproducible entry points - which typically are available through unit tests in most development environments - and their traceability, whereas the concrete tracing technique is of no significance.

Consequently, it is fair to say that our approach is applicable to the vast majority of object-oriented programming languages.
Furthermore, since the re-implementation for other languages and platforms would undoubtedly be a tedious task, it is worth mentioning that it has been proven to be feasible to embed the existing tools of the \textsc{PathTools} framework into other development environments \cite{richter_integration_2013}\todo{cite when available}.
The approach consists of a unified meta-model which is used to feed collected information from programs written in other target languages to the existing tools.
The only requirement is the implementation of a custom tracer and the conversion of the collected traces to the proposed meta-model.